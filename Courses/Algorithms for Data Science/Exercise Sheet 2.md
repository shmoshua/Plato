#Algorithms #Series 

#### 1. Maximum of Squared Components of a Gaussian Vector
1. We assume that we have a non-degenerate Gaussian distribution, i.e. $\Sigma_{ii}>0$ for all $i\in[d]$. Let $z:=\max_{i\in[d]} x_{i}^{2}$. Then, by Jensen for any $t>0$,$$\exp(t\mathbb{E}[z])\leq \mathbb{E}[\exp(tz)]=\int_{0}^{\infty} \mathbb{P}(\exp (tz)\geq \lambda) \, d\lambda $$where: $$\begin{align}\mathbb{P}(\exp(tz)\geq \lambda)=\mathbb{P}(\max_{i\in[d]}\exp(tx_{i}^{2})\geq \lambda)=\mathbb{P}(\exists i\in[d]: \exp(tx^2_{i})\geq \lambda)\leq \sum_{i\in[d]}\mathbb{P}(\exp(tx^2_{i})\geq \lambda)\end{align}$$by union bound. Then, for any $i\in[d]$,  we have that $y_{i}:=\frac{1}{\sqrt{ \Sigma_{ii} }}x_{i}\sim \mathcal{N}(0,1)$ and:$$\begin{align}\mathbb{P}(\exp(tx^2_{i})\geq \lambda)=\mathbb{P}\left( \left| x_{i} \right| \geq \sqrt{ \frac{\log \lambda}{t} } \right)=\mathbb{P}\left( \left| y_{i} \right| \geq \sqrt{ \frac{\log \lambda}{\Sigma_{ii}t} } \right)\leq 2e^{-\frac{\log \lambda}{2\Sigma_{ii}t}}\leq 2e^{-\frac{\log \lambda}{2t}}=2\lambda^{-1/2t}\end{align}$$Hence, by taking $t=1/4$, $$\exp(\mathbb{E}[z] / 4)=\int_{0}^{\infty} \mathbb{P}(\exp (tz)\geq \lambda) \, d\lambda \leq 1+2d\int_{1}^{\infty} \lambda^{-2} \, d\lambda=1+2d $$This shows that $\mathbb{E}[z]\leq 4\log(2d+1)=O(\log d)$.
2. As $x\mapsto x^{2}$ is convex, by Jensen:$$\left( \mathbb{E}\left[ \frac{1}{\sqrt{ n }}\max_{i\in[d]}\left| \braket{ X_{i} , w }  \right|  \right]  \right)^{2}\leq \mathbb{E}\left[ \frac{1}{n}\max_{i\in[d]}\braket{ X_{i} , w }   ^{2} \right]=\mathbb{E}\left[ \max_{i\in[d]}\left\langle \frac{1}{\sqrt{ n }}X_{i},w\right\rangle^{2} \right]$$Now, notice that $\braket{ \frac{1}{\sqrt{ n }}X_{i} ,  w}$ is Gaussian as it is a sum of independent Gaussian variables. Further, $$\mathbb{E}\left[ \left\langle \frac{1}{\sqrt{ n }}X_{i},w\right\rangle\right] =\frac{1}{\sqrt{ n }}X_{i}^\top \mathbb{E}[w]=0,\quad\text{Var}\left( \frac{1}{\sqrt{ n }}X_{i}^\top w \right)=\frac{1}{n}X_{i}^\top X_{i}=\frac{1}{n}\left\| X_{i} \right\| ^2_{2}=1$$Hence, by 1, we get that: $\left( \mathbb{E}\left[ \frac{1}{\sqrt{ n }}\max_{i\in[d]}\left| \braket{ X_{i} , w }  \right|  \right]  \right)^{2}\leq O(\log d)$. Finally, $$\mathbb{E}\left[\max_{i\in[d]}\left| \braket{ X_{i} , w }  \right|  \right]  \leq \sqrt{ n }\cdot O(\log d)^{1/2}$$

---
#### 2. Ridge Regression
1. Consider the objective function $$f:\mathbb{R}^d\to \mathbb{R},\quad \beta\mapsto \left\| X\beta-y \right\| ^{2}+\lambda \left\| \beta \right\| ^2$$Then, $$\nabla f(\beta)=2X^\top (X\beta-y)+2\lambda\beta=2(n+\lambda)\beta-2X^\top y$$By equating it with zero, we get that: $$\widehat{\beta}_{\text{ridge},\lambda}(y)=\frac{1}{n+\lambda}X^\top y$$
2. We have that: $$\mathbb{E}[\widehat{\beta}_{\text{ridge},\lambda}(y)]=\frac{1}{n+\lambda}X^\top(X\beta ^{*}+\mathbb{E}[w])=\frac{n}{n+\lambda}\beta ^{*}$$Hence, $$\left\| \beta ^{*}-\mathbb{E}[\widehat{\beta}_{\text{ridge},\lambda}(y)] \right\|=\left\| \beta ^{*}-\frac{n}{n+\lambda}\beta ^{*}\right\| = \frac{\lambda}{n+\lambda}\left\| \beta ^{*}\right\| $$
3. We have: $$\begin{align}\mathbb{E}\left[ \left\| \widehat{\beta}_{\text{ridge},\lambda}(y)-\beta ^{*} \right\|^{2}  \right] &=\mathbb{E}\left[ \left\| \frac{n}{n+\lambda}\beta ^{*}+\frac{1}{n+\lambda}X^\top w-\beta ^{*} \right\|^{2}  \right] \\&=\mathbb{E}\left[ \left\| \frac{1}{n+\lambda}(X^\top w-\lambda\beta ^{*}) \right\|^{2}  \right] \\&=\left( \frac{1}{n+\lambda} \right) ^{2}\mathbb{E}[\left\| X^\top w -\lambda\beta ^{*}\right\| ^{2}]\\&=\left( \frac{1}{n+\lambda} \right) ^{2}(\mathbb{E}[\|X^\top w\|^2]+\lambda^{2}\left\| \beta ^{*} \right\| ^2)\end{align}$$We can conclude by noting: $$\mathbb{E}[\left\| X^\top w \right\| ^2]=\mathbb{E}[\text{tr}(w^\top XX^\top w)]=\mathbb{E}[\text{tr}(XX^\top ww^\top)]=\text{tr}(XX^\top)=\text{tr}(X^\top X)=\text{tr}(nI_{d})=nd$$
4. By law of total expectation, $$\begin{align} \mathbb{E}[\left\| \widehat{\beta} \right\| ]\end{align}$$
