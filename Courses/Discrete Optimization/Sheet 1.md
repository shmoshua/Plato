#Series #DiscreteOptimization

##### Problem 1.1: Carathéodory’s Theorem
1. Firstly, one notices that it is a polyhedron. Further, as $Q$ is contained in the unit ball w.r.t. $\|\cdot\|_{\infty}$, it is clearly a polytope. Hence, it suffices to show that it is non-empty.
   
   Let $P:=\{ x\in \mathbb{R}^n: Ax \leq b  \}$ and we define $A_{=}$ as the constraints where $A_{=}x = b_{=}$. We proceed via induction over $n - \text{rnk}(A_{=})$. If $\text{rnk}(A_{=})=n$, then $x\in V$ and $e_{x}\in Q$. Now, suppose $\text{rnk}(A_{=})<n$, then there exists $0\ne z\in \text{ker }A_{=}$. Further, as $P$ is bounded, there exists $\alpha<0,\beta>0$ s.t. $x+az\in P$ if and only if $\alpha\leq a \leq \beta$. Then, $$x=\frac{\beta}{\beta-\alpha}(x+\alpha z)+\frac{-\alpha}{\beta-\alpha}(x+\beta z)$$and $x$ is a convex combination of two points in $P$. It suffices to show that $x+\alpha z,x+\beta z$ are convex combinations of the vertices. By definition, there is a row $a_{i}^\top$in $A$ s.t. $a_{i}^\top x < b_{i}$ but $a_{i}^\top(x+\alpha z)=b_{i}$. Hence, by induction we have that $x+\alpha z$ is a convex combination and similarly is $x+\beta z$. This concludes the proof.
1. As $Q$ is a non-empty polytope, it has a vertex $\lambda ^{*}$. We can represent $Q$ in standard form LP, i.e. $Q:=\{ \lambda\in \mathbb{R}^{|V|}_{+}:C\lambda=d  \}$ where $C\in \mathbb{R}^{n+1,|V|}$ and $d\in \mathbb{R}^{n+1}$. Note that if $|V|\leq n+1$ the statement holds trivially. Suppose otherwise. As $\lambda ^{*}$ is a vertex of $Q$, there exists $B\in {|V| \choose n+1}$ s.t. $\text{supp}(\lambda^*)\subseteq B$ and $\lambda ^{*}=C_{B}^{-1} d$. Therefore, $$x=\sum_{v\in V}^{}\lambda ^{*}_{v}v=\sum_{v\in B}^{}\lambda ^{*}_{v}v$$where $\sum_{v\in B}^{}\lambda ^{*}_{v}=\sum_{v\in V}^{}\lambda ^{*}_{v}=1$. This shows the statement.
---
##### Problem 1.2: Rank Deficiency in Unbounded Polyhedra
$A$ not having a full column rank is equivalent to there being a non-zero $v\in \mathbb{R}^n$ s.t. $Av = 0$. Let $P$ contain a line. Then, there exists $0\ne v\in \mathbb{R}^n$ and $x\in \mathbb{R}^{n}$ s.t. $x+\mathbb{R} v\subseteq P$. Then, we have that: $$Ax+\lambda A v\leq b,\quad \forall \lambda\in \mathbb{R}$$and this implies that $Av = 0$. 

Conversely, let $0\ne v\in \text{ker } A$. Then, let $x\in P$, which exists as $P$ is non-empty. Then for any $\lambda\in \mathbb{R}$, $$A(x+\lambda v)=Ax\leq b$$This shows that $P$ contains a line.

---
##### Problem 1.3: Strong Duality and Farkas Lemma
1. Let $P:=\{  x\in \mathbb{R}^n: Ax\leq 0 \}$ and $Q:=\{ y\in \mathbb{R}^m_{+}: A^\top y = c \}$. Note that $0\in P$ and $P$ is hence non-empty. 
   
   Assume that there exists $y\in \mathbb{R}^m_{+}$ s.t. $A^\top y = c$, i.e. $Q$ is non-empty. Then, the dual polyhedron is non-empty and by setting $b = 0$, we have that: $\max\{ c^\top x : Ax\leq 0,x\in \mathbb{R}^n \} = 0$ by the strong LP duality. 
   
   Now, assume that there doesn't exist such a $y$, i.e. $Q$ is empty. Then, $c\notin \text{cone}(A)$, i.e. the cone generated by the rows of $A$. By the separating hyperplane theorem from the script, there exists $x\in \mathbb{R}^m$ s.t. $a^\top x\geq 0$ for all $a\in \text{cone}(A)$ and $c^\top x <0$. By multiplying $x$ by $-1$, we get that: $$Ax \leq 0,\quad c^\top x>0$$This shows the statement.
1. Assume that $P:=\{ x\in \mathbb{R}^n:Ax\leq b \}$ and $Q:=\{ y\in \mathbb{R}^m_{+}: A^\top y = c \}$ are both non-empty. Let $x^{*}$ be an optimal solution of $\max_{x\in P}c^\top x$. Let $A_{=}$ be the rows of $A$ where the constraints are tight and $A_{<}$ the remaining rows. We want to show via Farkas Lemma that there exists $y^*_{=}\geq 0$ s.t. $A^\top_{=}y^{*}_{=}=c$. Assume otherwise. Then, by Farkas, there exists $x\in \mathbb{R}^n$ s.t. $A_{=}x \leq 0$ and $c^\top x > 0$. Now, let $\varepsilon>0$ and set $x':= x^{*}+\varepsilon x$. Then, $$A_{=}x'=A_{=}x^{*}+\varepsilon A_{=}x\leq A_{=}x^{*}=b_{=}$$and $$A_{<}x'=A_{<}x^{*}+\varepsilon A_{<}x<b_{<}$$for some small $\varepsilon$. Hence, $x'\in P$. However, $$c^\top x'=c^\top x^{*}+\varepsilon c^\top x>c^\top x^{*}$$which is a contradiction. This shows that there exists such $y^{*}_{=}\geq 0$ with $A^\top_{=}y^{*}_{=}=c$. By setting $y^{*}:=(y^{*}_{=},0)$, i.e. filling the remaining entries with zero, $y^{*}\geq 0$ and $A^\top y^{*}=c$. More importantly, $$b^\top y^{*}= b_{=}^\top  y^{*}_{=}=(x^{*})^\top A_{=}^\top y^{*}_{=}=c^\top x^{*}$$

---
##### Problem 1.4: Totally unimodular characterization
1. Let $A$ be totally unimodular. Firstly, $[A|I]$ is clearly full row-rank. Now, let $B\in {[n+m] \choose m}$ s.t. $[A|I]_{B}$ is invertible. Let $B':= B\cap [n]$. As the norm of the determinant is invariant under row swaps, there is a block matrix $A'$ s.t. $$A'=\begin{bmatrix} C&0\\D&I_{m-\left| B' \right| }\end{bmatrix}$$and $\left| \det[A|I]_{B} \right|=\left| \det A' \right|=\left| \det C \right|$. However, $C$ is also a square submatrix of $A$ modulo row swaps. Hence, $\det([A|I]_{B})\in \{ \pm 1 \}$.
   
    Conversely, let $k$ be the index and $I\in {[m] \choose k},J\in {[n] \choose k}$ be the row/column index sets respectively. Then, we aim to show that $\det A_{IJ}\in \{ -1,0,1 \}$. If $\det A_{IJ}=0$ we are done. Suppose $A_{IJ}$ is invertible. We construct a matrix $M$ where $$M:=[A_{J}|(I_{m})_{[m] \backslash I}]$$Then, similarly as above $\left| \det A_{IJ} \right|=\left| \det M \right|\in \{ \pm 1 \}$.
1. Suppose $A$ is totally unimodular. Let $k\leq \min\{ 4m,n \}$ be the index and $I$ and $J$ be index sets of size $k$. Then, for $M:=\begin{bmatrix} A\\-A\\I\\-I\end{bmatrix}$, we want to show that $M_{IJ}$ has determinant in $\{ -1, 0 , 1 \}$. If $M_{IJ}$ has linearly dependent rows, the determinant is zero. Hence, with the same logic as above we can show it for the other cases and it suffices to consider the case where $I\subseteq [m]$. However, then $\det M_{IJ}\in \{ -1,0,1 \}$ from the total unimodularity of $A$. The converse is trivial as any square submatrix in $A$ is also a square submatrix in $M$.
2. Holds by the invariance of determinant under transposition.

---
