#Definition #InformationTheory 

> [!definition]
> For discrete random variables $X,Y$ taking values in $\mathcal{X}$ and $\mathcal{Y}$ respectively, 
> 1. the ***joint entropy*** with a joint distribution $p(x,y)$ is given as: $$H(X,Y)=-\sum_{x,y}^{}p(x,y)\log p(x,y)=-\mathbb{E}(\log p(X,Y))$$
- **Remark**: Joint entropy is symmetric.
---
##### Properties

> [!lemma] Proposition 1 (Basic Properties)
> We have:
> 1. $H(X,Y)=H(Y,X)$
> 