#Definition #InformationTheory 

> [!definition]
> For discrete random variables $X,Y$ taking values in $\mathcal{X}$ and $\mathcal{Y}$ respectively, 
> 1. the ***joint entropy*** with a joint distribution $p(x,y)$ is given as: $$H(X,Y)=-\sum_{x,y}^{}p(x,y)\log p(x,y)=-\mathbb{E}(\log p(X,Y))$$

^67ab3f

- **Remark**: Joint entropy is symmetric. ^0aadb8
---
##### Properties
